# ğŸ“Š Pulselytics â€” Professional Social Media Analytics Dashboard

> **Full-stack analytics platform for advertising agencies and social media managers.** Track engagement, analyze trends, and visualize performance across Instagram, Facebook, YouTube, and Twitter/X â€” all without official APIs.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![React](https://img.shields.io/badge/react-18-blue.svg)
![Node](https://img.shields.io/badge/node-18+-green.svg)

---

## âœ¨ Features

### ğŸ“Š **Professional Dashboard**
- **Modern UI** â€” Clean, data-focused design with Tailwind CSS
- **Real-time Analytics** â€” Live engagement metrics and trend analysis
- **Interactive Charts** â€” Recharts visualizations with smooth animations
- **Multi-client Management** â€” Switch between clients with dynamic data updates
- **Platform Filtering** â€” View Instagram, Facebook, YouTube, Twitter/X independently
- **Date Range Selection** â€” Analyze 7/30/90 days or custom ranges
- **Search & Filter** â€” Find posts by text, hashtags, or platform
- **ğŸŒ™ Dark Mode** â€” Complete dark theme with toggle and localStorage persistence

### ğŸ“ˆ **Advanced Analytics**
- **Engagement Trends** â€” Line charts showing likes, comments, views over time
- **Top Posts** â€” Ranked grid of highest-performing content
- **Hashtag Insights** â€” Bar chart of most-used hashtags
- **Platform Distribution** â€” Compare post count across social channels
- **Sentiment Analysis** â€” VADER-powered positive/neutral/negative breakdown
- **Content Type Analysis** â€” Photo vs video vs text-only distribution
- **Posting Frequency** â€” Heatmap of optimal posting times
- **âœ¨ AI-Powered Insights** â€” GPT-3.5-turbo analytics with rule-based fallback

### ğŸ¤– **AI & Machine Learning Features** (NEW!)
- **ğŸ¯ Predictive Engagement** â€” Predict post performance before publishing (ML-powered)
- **ğŸš¨ Anomaly Detection** â€” Auto-detect viral posts, engagement drops, and unusual patterns
- **â° Optimal Time Predictor** â€” ML-based recommendations for best posting times
- **ğŸ“ˆ Trend Forecasting** â€” Predict engagement trends 7 days ahead
- **ğŸ§  Custom ML Models** â€” Train personalized models on each client's historical data
- **ğŸ“Š Virality Scoring** â€” Rate content potential on a 0-100 scale
- **ğŸ’¡ Smart Recommendations** â€” Data-driven content strategy suggestions

### ğŸ“„ **Reports & Export**
- **PDF Report Generation** â€” Professional reports with charts and analytics
- **CSV Export** â€” Download data for external analysis
- **JSON Export** â€” API-ready data format
- **Automated Insights** â€” AI-generated recommendations and trends

### ğŸ”„ **Web Scraping (No APIs Required)**
- **âš¡ Lightweight Scraper** â€” Fast API-based scraping (3-10s total)
  - Instagram: `instaloader` (public profiles)
  - YouTube: `yt-dlp` (channel metadata)
  - Twitter/X: `snscrape` (public tweets)
  - Facebook: Best-effort public page scraping
- **ğŸŒ Playwright Scraper** â€” Browser automation fallback (15-30s)
  - All platforms with login support
  - Proxy integration for production
  - Anti-detection features

### ğŸ› ï¸ **Technical Stack**

**Frontend:**
- React 18 + Vite
- Tailwind CSS for styling
- Recharts for data visualization
- React Router for navigation
- Axios for API calls
- Lucide React icons

**Backend:**
- Flask REST API
- Flask-CORS for cross-origin requests
- Pandas for data processing
- VADER Sentiment for NLP
- CSV/JSON data storage

---

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.9+** with pip
- **Node.js 18+** with npm

### 1ï¸âƒ£ **Automated Setup** (Recommended)

```powershell
cd c:\pulselytics
.\setup.ps1
```

This script will:
- Create Python virtual environments
- Install all backend and frontend dependencies
- Create `.env` configuration files
- Display next steps

### 2ï¸âƒ£ **Manual Setup**

#### Backend

```powershell
cd c:\pulselytics\backend

# Create and activate virtual environment
python -m venv venv
.\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
copy .env.example .env

# Start server
python app.py
```

**Backend runs on:** `http://127.0.0.1:5000`

#### Frontend

```powershell
cd c:\pulselytics\frontend

# Install dependencies
npm install

# Configure environment
copy .env.example .env

# Start development server
npm run dev
```

**Frontend runs on:** `http://localhost:5173`

### 3ï¸âƒ£ **Run Scrapers**

```powershell
cd c:\pulselytics\scripts
..\venv\Scripts\activate

# Scrape YouTube (most reliable)
python scrape_youtube.py --channel NASA --max-videos 20

# Scrape Instagram (small batches to avoid rate limits)
python scrape_instagram.py --username nasa --max-posts 10

# Scrape Twitter/X
python scrape_twitter.py --username NASA --max-posts 30
```

### 4ï¸âƒ£ **Access Dashboard**

Open `http://localhost:5173` in your browser

---

## ğŸ“ Project Structure

```
pulselytics/
â”œâ”€â”€ frontend/                   # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # Reusable UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ Layout.jsx     # Main layout with sidebar + header
â”‚   â”‚   â”‚   â”œâ”€â”€ KPICard.jsx    # Metric cards
â”‚   â”‚   â”‚   â”œâ”€â”€ ChartTooltip.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ LoadingSkeleton.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ProfileAutocomplete.jsx
â”‚   â”‚   â”œâ”€â”€ pages/             # Page components
â”‚   â”‚   â”‚   â”œâ”€â”€ Overview.jsx   # Dashboard with KPIs and charts
â”‚   â”‚   â”‚   â”œâ”€â”€ Analytics.jsx  # Detailed analytics
â”‚   â”‚   â”‚   â”œâ”€â”€ TopPosts.jsx   # Top performing posts
â”‚   â”‚   â”‚   â”œâ”€â”€ Reports.jsx    # Export functionality
â”‚   â”‚   â”‚   â”œâ”€â”€ Settings.jsx   # Client management + scraper controls
â”‚   â”‚   â”‚   â””â”€â”€ APIKeys.jsx    # API key management
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ api.js         # Centralized API calls
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ exportData.js  # Data export utilities
â”‚   â”‚   â”œâ”€â”€ App.jsx            # Main app component
â”‚   â”‚   â””â”€â”€ main.jsx           # Entry point
â”‚   â”œâ”€â”€ package.json           # Frontend dependencies
â”‚   â”œâ”€â”€ vite.config.js         # Vite configuration
â”‚   â””â”€â”€ tailwind.config.js     # Tailwind CSS configuration
â”‚
â”œâ”€â”€ backend/                    # Flask API server
â”‚   â”œâ”€â”€ app.py                 # Main Flask application
â”‚   â”œâ”€â”€ database.py            # SQLite database utilities
â”‚   â”œâ”€â”€ encryption.py          # API key encryption
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”œâ”€â”€ pulselytics.db         # SQLite database
â”‚   â””â”€â”€ data/                  # Client JSON files
â”‚       â”œâ”€â”€ mrbeast.json       # Sample: MrBeast
â”‚       â”œâ”€â”€ nike.json          # Sample: Nike
â”‚       â”œâ”€â”€ adidas.json        # Sample: Adidas
â”‚       â”œâ”€â”€ redbull.json       # Sample: Red Bull
â”‚       â”œâ”€â”€ gopro.json         # Sample: GoPro
â”‚       â””â”€â”€ nasa_test.json     # Sample: NASA
â”‚
â”œâ”€â”€ scripts/                    # Scraper and utility scripts
â”‚   â”œâ”€â”€ scrape_instagram.py    # Instagram scraper
â”‚   â”œâ”€â”€ scrape_instagram_api.py # Instagram Graph API scraper
â”‚   â”œâ”€â”€ scrape_youtube.py      # YouTube scraper
â”‚   â”œâ”€â”€ scrape_youtube_api.py  # YouTube Data API v3 scraper
â”‚   â”œâ”€â”€ scrape_twitter.py      # Twitter/X scraper
â”‚   â”œâ”€â”€ scrape_twitter_api.py  # Twitter API v2 scraper
â”‚   â”œâ”€â”€ scrape_facebook.py     # Facebook scraper
â”‚   â”œâ”€â”€ scrape_facebook_api.py # Facebook Graph API scraper
â”‚   â”œâ”€â”€ analyze_data.py        # Analytics engine
â”‚   â”œâ”€â”€ common.py              # Shared utilities
â”‚   â”œâ”€â”€ seed_demo_data.py      # Demo data generator
â”‚   â””â”€â”€ setup.ps1              # Automated setup script
â”‚
â”œâ”€â”€ data/                       # Scraped data (CSV)
â”‚   â”œâ”€â”€ instagram_data.csv
â”‚   â”œâ”€â”€ youtube_data.csv
â”‚   â”œâ”€â”€ twitter_data.csv
â”‚   â””â”€â”€ facebook_data.csv
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ API_SETUP.md           # API setup guides
â”‚   â”œâ”€â”€ API_INTEGRATION_GUIDE.md
â”‚   â”œâ”€â”€ DATABASE_SETUP.md
â”‚   â”œâ”€â”€ INSTAGRAM_API_SETUP.md
â”‚   â”œâ”€â”€ ANALYTICS_ENHANCEMENTS.md
â”‚   â”œâ”€â”€ NEW_FEATURES.md
â”‚   â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md
â”‚   â”œâ”€â”€ SETUP_COMPLETE.md
â”‚   â””â”€â”€ SETUP_SUMMARY.md
â”‚
â”œâ”€â”€ venv/                       # Python virtual environment
â”‚
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ QUICKSTART.md               # Quick setup guide
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ cleanup.ps1                 # Organization script
â””â”€â”€ .gitignore                  # Git ignore rules
```

---

## ğŸ”Œ API Endpoints

### Client Management
| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/clients` | List all clients |
| `GET` | `/api/clients/:id` | Get client by ID |
| `POST` | `/api/clients` | Create new client |
| `PUT` | `/api/clients/:id` | Update client |
| `DELETE` | `/api/clients/:id` | Delete client |

### Analytics & Data
| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/analytics` | Get analytics (supports `?client=&range=&platform=`) |
| `GET` | `/api/clients/:id/posts` | Get posts for client |
| `GET` | `/api/stats/summary` | Get overall summary stats |

### Reports & Insights
| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/reports/generate` | Generate PDF report |
| `GET` | `/api/reports/download/:filename` | Download PDF report |
| `POST` | `/api/insights/generate` | Generate AI insights |
| `POST` | `/api/insights/content-recommendations` | Get content recommendations |

### Scraping
| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/scrape` | Trigger scraping for client |
| `GET` | `/api/schedule/status` | Get scraper mode and schedule info |

### Health
| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/health` | Health check |

---

## âš™ï¸ Configuration

### Backend Environment (`backend/.env`)

```bash
# Scraper Mode: 'lightweight' (faster) or 'playwright' (all platforms)
SCRAPER_MODE=lightweight

# Flask Configuration
FLASK_DEBUG=True
PORT=5000

# Auto-scraping interval (minutes)
SCRAPE_INTERVAL_MINUTES=360

# OpenAI API Key (optional - for AI insights)
OPENAI_API_KEY=your-api-key-here

# Playwright Proxies (optional, for production)
# PROXY_SERVER=http://proxy.example.com:8080
# PROXY_USERNAME=your_username
# PROXY_PASSWORD=your_password
```

### Frontend Environment (`frontend/.env`)

```bash
# Backend API URL
VITE_API_URL=http://127.0.0.1:5000/api
```

---

## ğŸ“Š Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Scrapers      â”‚  â† Python scripts fetch public posts
â”‚  (Instagram, YouTube,â”‚    from social media platforms
â”‚   Twitter/X, FB)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CSV Files         â”‚  â† Data stored locally in /data/
â”‚ (instagram_data.csv,â”‚
â”‚  youtube_data.csv)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  analyze_data.py    â”‚  â† Processes data: engagement,
â”‚  (Analytics Engine) â”‚    sentiment, hashtags, trends
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Flask Backend     â”‚  â† Provides REST API endpoints
â”‚   (Port 5000)       â”‚    for frontend consumption
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend     â”‚  â† Dashboard displays charts,
â”‚  (Port 5173)        â”‚    KPIs, and top posts
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Scraper Details

| Platform | Lightweight Tool | Speed | Reliability | Rate Limits |
|----------|-----------------|-------|-------------|-------------|
| **YouTube** | `yt-dlp` | âš¡ 5-10s | â­â­â­â­â­ Excellent | Low |
| **Instagram** | `instaloader` | âš¡ 3-5s | â­â­â­ Good | **High** (5-10 posts/hour) |
| **Twitter/X** | `snscrape` | âš¡ 8-12s | â­â­â­â­ Very Good | Medium |
| **Facebook** | `requests` + BS4 | âš¡ 10-15s | â­â­ Fair | Very High |

### Best Practices

âœ… **Start with YouTube** â€” Most stable scraper  
âœ… **Instagram: Small batches** â€” Use `--max-posts 5-10` to avoid 401 errors  
âœ… **Wait between runs** â€” 30-60 min cooldown for Instagram  
âœ… **Public data only** â€” No login required, respects ToS  
âœ… **Monitor logs** â€” Check terminal output for errors  

---

## ğŸ§ª Testing

### Test Backend API

```powershell
# Health check
curl http://127.0.0.1:5000/api/health

# Get clients
curl http://127.0.0.1:5000/api/clients

# Get analytics
curl "http://127.0.0.1:5000/api/analytics?range=30days&platform=all"
```

### Test Scrapers

```powershell
# Quick test with NASA
python scrape_youtube.py --channel NASA --max-videos 5

# Check results
cat .\data\youtube_data.csv
```

---

## ğŸ“ˆ Dashboard Features

### Overview Page
- **4 KPI Cards** â€” Total Posts, Avg Likes, Avg Comments, Avg Views
- **Engagement Trend Chart** â€” Line graph of engagement over time
- **Platform Distribution** â€” Bar chart of posts per platform
- **Top Hashtags** â€” Horizontal bar chart of most-used hashtags
- **Top Posts Grid** â€” 6 highest-performing posts with thumbnails

### Settings Page
- **Scraper Status** â€” View current mode (lightweight/playwright)
- **Client Management** â€” Add, edit, delete clients
- **Scrape Triggers** â€” Manual scraping with "Scrape Now" button
- **Platform Configuration** â€” Set Instagram, YouTube, Facebook, Twitter handles

### Analytics Page
- Detailed trend analysis
- Sentiment breakdown (positive/neutral/negative)
- Content type distribution (photo/video/text)
- Posting frequency heatmap

---

## ğŸš¨ Troubleshooting

### Backend won't start
```powershell
# Check Python version
python --version  # Must be 3.9+

# Reinstall dependencies
cd backend
.\venv\Scripts\activate
pip install -r requirements.txt

# Check port availability
netstat -ano | findstr :5000
```

### Frontend won't start
```powershell
# Check Node version
node --version  # Must be 18+

# Clear and reinstall
cd frontend
Remove-Item -Recurse -Force node_modules
npm install
```

### Instagram 401 Errors
- **Cause:** Rate limiting after 10-15 posts
- **Solution:** Use `--max-posts 5`, wait 30-60 min between runs
- **Alternative:** Switch to Playwright scraper mode

### No data in dashboard
- **Cause:** Scrapers haven't run yet
- **Solution:** Run scrapers to populate `/data/*.csv` files
- **Check:** Backend logs for CSV loading errors

---

## ğŸ“… Automation (Optional)

### Windows Task Scheduler

Create a scheduled task to run scrapers daily:

```powershell
# Create task to run daily at 6 AM
schtasks /create /tn "Pulselytics Daily Scrape" /tr "C:\pulselytics\venv\Scripts\python.exe C:\pulselytics\update_all.py --instagram nasa --youtube NASA --twitter NASA" /sc daily /st 06:00
```

---

## ğŸ” Ethics & Legal

- **Public data only** â€” No login required, no private accounts
- **Rate limiting** â€” Respects platform limits with delays
- **ToS compliance** â€” Educational/analytics use only
- **No warranty** â€” Use at your own risk
- **Personal use** â€” Not for commercial redistribution

---

## ğŸ“š Documentation

- **[QUICKSTART.md](QUICKSTART.md)** â€” 5-minute setup guide
- **[setup.ps1](setup.ps1)** â€” Automated installation script
- **Backend API** â€” See docstrings in `backend/app.py`
- **Scraper Guides** â€” Comments in individual scraper files

---

## ğŸ”® Roadmap

- [x] Full-stack React + Flask architecture
- [x] Multi-client management
- [x] Real-time scraping with API triggers
- [x] Sentiment analysis with VADER
- [x] Hashtag analytics
I'll start the backend server and verify it's running with sample data.

Starting (1/2) Start backend server


cd c:\pulselytics\backend; ..\venv\Scripts\python.exe app.py
Let me check the terminal output to see if the server started successfully:

Checked background terminal output

The backend can't find common.py because we moved it to the scripts folder. I'll fix the import by updating the path in app.py:

Read app.py, lines 1 to 31

app.py+5-2
Now let me restart the backend server with the fixed imports:


..\venv\Scripts\python.exe app.py
Checked background terminal output


cd c:\pulselytics\backend; & ..\venv\Scripts\python.exe app.py
Checked background terminal output

Perfect! Backend is running. Now let me verify it's responding correctly:

Starting (2/2) Verify server response


Invoke-RestMethod -Uri 'http://127.0.0.1:5000/api/health' -UseBasicParsing

Start-Sleep -Seconds 2; try { $health = Invoke-RestMethod -Uri 'http://127.0.0.1:5000/api/health' -UseBasicParsing; Write-Host "âœ… Backend Health: $($health.status)" -ForegroundColor Green; $clients = Invoke-RestMethod -Uri 'http://127.0.0.1:5000/api/clients' -UseBasicParsing; Write-Host "âœ… Clients loaded: $($clients.count)" -ForegroundColor Green } catch { Write-Host "âŒ Backend not responding: $_" -ForegroundColor Red }
The server is terminating. Let me check if there's an issue with the data directory path. I notice it's looking for data but the data is in data. Let me fix the common.py path:

Read common.py, lines 1 to 21

common.py+2-1
Now restart the backend:


& ..\venv\Scripts\python.exe app.py

cd c:\pulselytics\backend; Start-Process -NoNewWindow -FilePath '..\venv\Scripts\python.exe' -ArgumentList 'app.py'

Start-Sleep -Seconds 3; try { $health = Invoke-RestMethod -Uri 'http://127.0.0.1:5000/api/health' -UseBasicParsing; Write-Host "`nâœ… Backend Status: $($health.status)" -ForegroundColor Green; Write-Host "âœ… Scraper Mode: $($health.scraper_mode)" -ForegroundColor Green; $clients = Invoke-RestMethod -Uri 'http://127.0.0.1:5000/api/clients' -UseBasicParsing; Write-Host "âœ… Sample Clients: $($clients.count)" -ForegroundColor Green; Write-Host "`nğŸš€ Backend running at http://127.0.0.1:5000" -ForegroundColor Cyan } catch { Write-Host "`nâŒ Backend not responding" -ForegroundColor Red; throw }

$health = Invoke-RestMethod -Uri 'http://127.0.0.1:5000/api/health' -UseBasicParsing; $health | ConvertTo-Json
Excellent! Backend is running. Now let me verify sample data:

Completed (2/2) Verify server response


$clients = Invoke-RestMethod -Uri 'http://127.0.0.1:5000/api/clients' -UseBasicParsing; Write-Host "`nâœ… PULSELYTICS BACKEND RUNNING" -ForegroundColor Green; Write-Host "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" -ForegroundColor Gray; Write-Host "Status: OK" -ForegroundColor Cyan; Write-Host "URL: http://127.0.0.1:5000" -ForegroundColor Cyan; Write-Host "Sample Clients: $($clients.count)" -ForegroundColor Cyan; Write-Host "Mode: lightweight" -ForegroundColor Cyan; Write-Host "`nClients:" -ForegroundColor Yellow; $clients.clients | ForEach-Object { Write-Host "  â€¢ $($_.name)" -ForegroundColor White }
Perfect! âœ… Backend server is running successfully!

Summary
âœ… Backend Server Started

URL: http://127.0.0.1:5000
Status: OK
Mode: lightweight scraper
Sample Clients: 6 loaded (Adidas, GoPro, MrBeast, NASA Test, Nike, Red Bull)
Fixed Issues:

Updated app.py to include scripts folder in Python path
Updated common.py to point DATA_DIR to data (one level up from scripts)
Next Step:
To start the frontend dashboard, run:


- [x] Platform filtering
- [x] PDF report generation
- [x] Dark mode
- [x] AI-powered insights (GPT integration)
- [ ] Email alerts for anomalies
- [ ] Competitor analysis
- [ ] Mobile responsive design

---

## ğŸ’¡ Pro Tips

1. **Start with sample clients** â€” Pre-loaded in `backend/data/`
2. **Use lightweight mode** â€” 3-5x faster than Playwright
3. **YouTube first** â€” Most reliable for testing
4. **Small Instagram batches** â€” Avoid rate limits
5. **Check browser console** â€” F12 for API errors
6. **Monitor backend logs** â€” Helpful debug messages

---

## ğŸ¤ Contributing

This is a personal analytics tool. Feel free to fork and customize for your needs!

---

## ğŸ“„ License

MIT License â€” Free to use for personal and client projects.

---

## ğŸ‘¨â€ğŸ’» Author

Built with â¤ï¸ for social media professionals managing clients at scale.

**Pulselytics** â€” _Pulse of your social presence._

---

## ğŸ†˜ Support

- **Issues:** Check terminal logs (backend and frontend)
- **Scrapers:** Review error messages, adjust rate limits
- **API:** Test endpoints with `curl` or Postman
- **Frontend:** Check browser console (F12) for React errors

For detailed setup help, see **[QUICKSTART.md](QUICKSTART.md)**

---

**Ready to start?** Run `.\setup.ps1` and you'll be analyzing social media data in 5 minutes! ğŸš€
